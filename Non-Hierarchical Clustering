#[1]비계층적 군집분석(Non-Hierarchical Clustering)

#중심 기반(Center-based) : K-means
#밀도 기반(Density-based) : DBSCAN

#==========1===========
#K-means: n개의 데이터와 k(<=n)개의 중심점(centroid)이 주어졌을때 각 그룹 내의 데이터와 중심점 간의 비용(거리)을 최소화하는 방향으로 계속 업데이트를 해줌으로써 그룹화를 수행하는 기법

# 1. 초기점(k) 설정

# k는 중심점(centroid)이자, 묶일 그룹(cluster)의 수와 같다.
# 위 예시에서는 k=3으로 설정(동그라미)
# 2. 그룹(cluster) 부여

# k개의 중심점(동그라미)과 개별 데이터(네모)간의 거리를 측정한다.
# 가장 가까운 중심점으로 데이터를 부여한다.
# 3. 중심점(centroid) 업데이트

# 할당된 데이터들의 평균값(mean)으로 새로운 중심점(centroid)을 업데이트한다.
# 4. 최적화

# 2,3번 작업을 반복적으로 수행한다.
# 변화가 없으면 작업을 중단한다.

from sklearn.cluster import KMeans

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import numpy as np


#클러스터의 개수 지정(n개)
num_clusters = n
#데이터셋 Z 삽입
km = KMeans(n_clusters=num_clusters)
km.fit(Z)


from scipy.spatial.distance import cdist
distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k, init='k-means++').fit(Z)
    kmeanModel.fit(Z)
    distortions.append(sum(np.min(cdist(Z, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / Z.shape[0])
# Plot the elbow
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

#==========2===========
#DBSCAN: 밀도기반(Density-based) 클러스터링 방법으로 “유사한 데이터는 서로 근접하게 분포할 것이다”는 가정을 기반으로, K-means와 달리 처음에 그룹의 수(k)를 설정하지 않고 자동적으로 최적의 그룹 수를 찾아나감
# 1. 먼저 하나의 점(파란색)을 중심으로 반경(eps) 내에 최소 점이 4개(minPts=4)이상 있으면, 하나의 군집으로 판단하며 해당 점(파란색)은 Core가 된다.
# 2. 반경 내에 점이 3개 뿐이므로 Core가 되진 않지만 Core1의 군집에 포함된 점으로, 이는 Border가 된다.
# 3. 1번과 마찬가지로 Core가 된다.
# 4. 반경내의 점중에 Core1이 포함되어 있어 군집을 연결하여 하나의 군집으로 묶인다.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
#Moon이라는 Data set 활용

x,y = make_moons(n_samples=300, noise=0.05, random_state=42)
df=pd.DataFrame(x)
df.head()

# 산점도
plt.figure(figsize=(7,5))
plt.title("Before", fontsize=15)
plt.plot(df[0], df[1], "o")
plt.grid()
plt.show()

#DBSCAN 함수
from sklearn.cluster import DBSCAN
db_scan = DBSCAN(eps=0.3, min_samples=5).fit(df.values)
df['cluster_db'] = db_scan.labels_

plt.figure(figsize=(7,5))
plt.title("After - DBSCAN", fontsize=15)
plt.scatter(df[0],df[1],c=df['cluster_db'])
plt.grid()
plt.show()



# 비계층적 군집분석 정리

# 1. K-means
# 군집의 수(k)를 미리 결정
# 중심점(Centroid) 갱신을 통해 비용함수를 최적화
# 이상치에 영향을 많이 받음

# 2. DBSCAN
# 반경(eps), 최소 개체 수(minPts)를 미리 결정
# 자동적으로 군집의 수 결정
# 군집 간 개체들이 섞이지 않음(최소 반경 내에 한해서)
# 노이즈 개념으로 이상치 검출이 가능



#==========3===========
#[2]계층적 군집분석(Hierarchical Clustering)
#K-평균 군집화(K-means Clustering)와 달리 군집 수를 사전에 정하지 않아도 학습을 수행할 수 있습니다. 개체들이 결합되는 순서를 나타내는 트리형태의 구조인 덴드로그램(Dendrogram) 이용

from scipy.cluster.hierarchy import dendrogram, ward1

# dataset
x, y = make_blobs(random_state=0, n_samples=12)

### 데이터 배열 x에 ward 함수를 적용
### scipy의 ward 함수는 병합 군집을 수행할 때 생성된 거리 정보가 담긴 배열을 반환

linkage_array = ward(x)
dendrogram(linkage_array)
ax = plt.gca() # get current axes
bounds = ax.get_xbound() # x축 데이터(처음과 끝), 즉 최소/최대값을 가진 (1,2)리스트
ax.plot(bounds, [7.25, 7.25], linestyle='--', c='k') # 임의로 라인 생성
ax.plot(bounds, [4, 4], linestyle='--', c='k')
ax.text(bounds[1], 7.25, ' 두 개의 클러스터', va='center', fontdict={'size':15}) # bounds: x축 끝
ax.text(bounds[1], 4, ' 세 개의 클러스터', va='center', fontdict={'size':15})
plt.xlabel('샘플 번호')
plt.ylabel('클러스터 거리')
plt.show()


# 계층적 클러스터링의 문제점

# 계층적 클러스터링의 최대 문제점은 greedy algorithm이라는 점이다. 한 번 병합이 되거나 분리된 군집은 다시 되돌릴 수 없다.
# 또한 앞서 알아본 것처럼 군집 간의 거리를 어떻게 계산하는지에 따라 노이즈와 아웃라이어에 취약하거나, 복잡한 군집을 다루지 못하는 등의 한계를 갖게 된다.
# 게다가 계층적 클러스터링은 상대적으로 계산 비용이 더 높다. Proximity matrix를 사용하기 때문에 데이터 포인트가 천개라면 그 제곱인 백만번을 계산해야한다.
